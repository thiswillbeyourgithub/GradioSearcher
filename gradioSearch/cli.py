"""
Main module for gradioSearch CLI tool
"""

import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import json

# from langchain_community.vectorstores import FAISS
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
from langchain_core.embeddings import Embeddings
from .utils.gui import launch_gui
from .utils.compressed_faiss import CompressedFAISS as FAISS


class SentenceTransformerEmbeddings(Embeddings):
    """Wrapper for SentenceTransformer to make it compatible with LangChain Embeddings interface.

    This is needed because FAISS expects an Embeddings object with embed_query() and
    embed_documents() methods, while SentenceTransformer uses a different API.
    """

    def __init__(self, model_name: str, **encode_kwargs):
        """
        Initialize with model name and optional encoding kwargs.

        Parameters
        ----------
        model_name : str
            Name of the sentence-transformers model to load
        **encode_kwargs : dict
            Additional keyword arguments to pass to model.encode()
            e.g., batch_size, show_progress_bar, normalize_embeddings, etc.
        """
        self.model = SentenceTransformer(model_name)
        self.encode_kwargs = encode_kwargs

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        Embed a list of documents.

        Parameters
        ----------
        texts : List[str]
            List of documents to embed

        Returns
        -------
        List[List[float]]
            List of embeddings, one per document
        """
        return self.model.encode(
            texts, convert_to_tensor=False, **self.encode_kwargs
        ).tolist()

    def embed_query(self, text: str) -> List[float]:
        """
        Embed a single query.

        Parameters
        ----------
        text : str
            Query text to embed

        Returns
        -------
        List[float]
            Query embedding
        """
        return self.model.encode([text], convert_to_tensor=False, **self.encode_kwargs)[
            0
        ].tolist()


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="CLI tool for searching FAISS vector databases with Gradio GUI"
    )

    parser.add_argument(
        "--db_path",
        type=str,
        required=True,
        help="Path to the Langchain FAISS database",
    )

    parser.add_argument(
        "--embedding_model",
        type=str,
        required=True,
        help="Embedding model name for sentence-transformers",
    )

    parser.add_argument(
        "--metadata_keys",
        type=str,
        default="*",
        help="Comma-separated list of metadata columns to display, or '*' to use all available keys (default: '*')",
    )

    parser.add_argument(
        "--topk",
        type=int,
        default=50,
        help="Number of top results to retrieve (default: 50)",
    )

    parser.add_argument(
        "--convert-embeddings",
        action="store_true",
        help="Convert embeddings: re-compute all embeddings using the specified model and save to output path",
    )

    parser.add_argument(
        "--output-db-path",
        type=str,
        default=None,
        help="Output path for converted FAISS database (required when --convert-embeddings is used)",
    )

    parser.add_argument(
        "--embedding-kwargs",
        type=str,
        default="{}",
        help='JSON string of kwargs to pass to the embedder, e.g. \'{"batch_size": 32, "normalize_embeddings": true}\'',
    )

    return parser.parse_args()


def convert_embeddings(
    vectorstore: FAISS,
    embedding_model: SentenceTransformerEmbeddings,
    output_path: str,
    batch_size: int = 32,
) -> None:
    """
    Extract all documents from vectorstore and re-compute embeddings.

    Parameters
    ----------
    vectorstore : FAISS
        Source FAISS vectorstore to extract documents from
    embedding_model : SentenceTransformerEmbeddings
        Embedding model to use for re-computation
    output_path : str
        Path to save the new FAISS database
    batch_size : int
        Batch size for embedding computation (default: 32)
    """
    print("\n=== Starting Embedding Conversion ===")

    # Extract all documents from the vectorstore's docstore
    print("Extracting documents from vectorstore...")
    documents = []
    if hasattr(vectorstore, "docstore") and hasattr(vectorstore.docstore, "_dict"):
        documents = list(vectorstore.docstore._dict.values())
    else:
        raise ValueError("Cannot access docstore to extract documents")

    print(f"Found {len(documents)} documents to re-embed")

    if not documents:
        print("No documents found. Aborting conversion.")
        return

    # Extract texts for embedding
    texts = [doc.page_content for doc in documents]

    # Re-compute embeddings in batches with progress bar
    print(f"Re-computing embeddings in batches of {batch_size}...")
    all_embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding batches"):
        batch_texts = texts[i : i + batch_size]
        batch_embeddings = embedding_model.embed_documents(batch_texts)
        all_embeddings.extend(batch_embeddings)

    print(f"Generated {len(all_embeddings)} embeddings")

    # Create new FAISS vectorstore from documents and embeddings
    print("Creating new FAISS vectorstore...")
    new_vectorstore = FAISS.from_embeddings(
        text_embeddings=list(zip(texts, all_embeddings)),
        embedding=embedding_model,
        metadatas=[doc.metadata for doc in documents],
    )

    # Save the new vectorstore
    print(f"Saving converted vectorstore to: {output_path}")
    output_path_obj = Path(output_path)
    output_path_obj.parent.mkdir(parents=True, exist_ok=True)
    new_vectorstore.save_local(output_path)

    print(f"âœ“ Conversion complete! New database saved to: {output_path}")


def main():
    """Main entry point"""
    try:
        args = parse_args()
    except SystemExit:
        return 1

    # Validate db_path exists
    db_path = Path(args.db_path)
    if not db_path.exists():
        print(f"Error: Database path '{args.db_path}' does not exist")
        return 1

    # Validate db_path is a directory
    if not db_path.is_dir():
        print(f"Error: Database path '{args.db_path}' is not a directory")
        return 1

    # Validate topk is positive
    if args.topk <= 0:
        print(f"Error: topk must be positive, got {args.topk}")
        return 1

    # Parse and validate metadata keys
    metadata_keys_input = args.metadata_keys.strip()
    if metadata_keys_input == "*":
        # Will be determined after loading the database
        metadata_keys = ["*"]
    else:
        try:
            metadata_keys = [
                key.strip() for key in metadata_keys_input.split(",") if key.strip()
            ]
            if not metadata_keys:
                print("Error: metadata_keys cannot be empty")
                return 1
        except Exception as e:
            print(f"Error parsing metadata_keys: {e}")
            return 1

    print(f"Database path: {args.db_path}")
    print(f"Embedding model: {args.embedding_model}")
    print(f"Metadata keys: {metadata_keys}")
    print(f"Top-k results: {args.topk}")

    # Validate conversion mode arguments
    if args.convert_embeddings:
        if not args.output_db_path:
            print("Error: --output-db-path is required when using --convert-embeddings")
            return 1

        output_path = Path(args.output_db_path)
        if output_path.exists():
            print(
                f"Warning: Output path '{args.output_db_path}' already exists and will be overwritten"
            )

    try:
        # Parse embedding kwargs
        try:
            embedding_kwargs = json.loads(args.embedding_kwargs)
            if not isinstance(embedding_kwargs, dict):
                print("Error: --embedding-kwargs must be a JSON object/dict")
                return 1
            print(f"Embedding kwargs: {embedding_kwargs}")
        except json.JSONDecodeError as e:
            print(f"Error parsing --embedding-kwargs JSON: {e}")
            return 1

        # Load embedding model
        print(f"Loading embedding model: {args.embedding_model}")
        try:
            embedding_model = SentenceTransformerEmbeddings(
                args.embedding_model, **embedding_kwargs
            )
        except Exception as e:
            print(f"Error loading embedding model '{args.embedding_model}': {e}")
            print("Please check that the model name is correct and available")
            return 1

        # Load FAISS database
        print(f"Loading FAISS database from: {args.db_path}")
        try:
            vectorstore = FAISS.load_local(
                args.db_path, embedding_model, allow_dangerous_deserialization=True
            )
        except Exception as e:
            print(f"Error loading FAISS database from '{args.db_path}': {e}")
            print(
                "Please check that the path contains a valid Langchain FAISS database"
            )
            return 1

        # Handle conversion mode
        if args.convert_embeddings:
            try:
                # Extract batch_size from embedding_kwargs if provided, otherwise use default
                batch_size = embedding_kwargs.get("batch_size", 32)
                convert_embeddings(
                    vectorstore=vectorstore,
                    embedding_model=embedding_model,
                    output_path=args.output_db_path,
                    batch_size=batch_size,
                )
                print("\n=== Conversion completed successfully ===")
                return 0
            except Exception as e:
                import traceback

                print(f"Error during conversion: {e}")
                print(f"Traceback:\n{traceback.format_exc()}")
                return 1

        # If metadata_keys is '*', extract all unique keys from the database
        if metadata_keys == ["*"]:
            print("Extracting all metadata keys from database...")
            try:
                all_keys = set()

                # Try to access docstore directly for efficient sampling
                if hasattr(vectorstore, "docstore") and hasattr(
                    vectorstore.docstore, "_dict"
                ):
                    # Sample documents from docstore
                    count = 0
                    for doc_id, doc in vectorstore.docstore._dict.items():
                        if count >= 100:  # Sample up to 100 documents
                            break
                        if hasattr(doc, "metadata") and doc.metadata:
                            all_keys.update(doc.metadata.keys())
                        count += 1
                else:
                    # Fallback: do a dummy search to get sample documents
                    try:
                        sample_docs = vectorstore.similarity_search("sample", k=50)
                        for doc in sample_docs:
                            if doc.metadata:
                                all_keys.update(doc.metadata.keys())
                    except:
                        pass

                if not all_keys:
                    print(
                        "Warning: No metadata keys found in database. Using empty list."
                    )
                    metadata_keys = []
                else:
                    metadata_keys = sorted(list(all_keys))
                    print(f"Found metadata keys: {metadata_keys}")
            except Exception as e:
                print(f"Error extracting metadata keys: {e}")
                return 1

        # Create search function with error handling
        def search_function(query: str, topk: int = args.topk) -> List[Dict[str, Any]]:
            try:
                # For empty queries, use a space to get initial documents
                search_query = query.strip() if query and query.strip() else " "

                # Perform similarity search
                print(f"DEBUG: Performing search for: '{search_query}' with k={topk}")
                docs_with_scores = vectorstore.similarity_search_with_score(
                    search_query, k=topk
                )
                print(f"DEBUG: Got {len(docs_with_scores)} results")

                results = []
                for i, (doc, score) in enumerate(docs_with_scores):
                    print(
                        f"DEBUG: Processing result {i}, score={score}, doc type={type(doc)}"
                    )
                    print(f"DEBUG: doc.page_content type={type(doc.page_content)}")
                    print(
                        f"DEBUG: doc.metadata type={type(doc.metadata)}, value={doc.metadata}"
                    )

                    result = {
                        "similarity_score": float(score),
                        "content": str(doc.page_content) if doc.page_content else "",
                        "metadata": dict(doc.metadata)
                        if isinstance(doc.metadata, dict)
                        else {},
                    }
                    results.append(result)

                print(f"DEBUG: Returning {len(results)} formatted results")
                return results
            except Exception as e:
                import traceback

                print(f"Error during search: {e}")
                print(f"Traceback:\n{traceback.format_exc()}")
                return []

        # Launch GUI
        print("Starting Gradio interface...")
        try:
            launch_gui(search_function, metadata_keys, args.topk)
        except Exception as e:
            print(f"Error launching GUI: {e}")
            return 1

    except KeyboardInterrupt:
        print("\nShutting down...")
        return 0
    except Exception as e:
        print(f"Unexpected error: {e}")
        return 1

    return 0
